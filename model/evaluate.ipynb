{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model\n",
    "The metric used to evaluate the model is the mean squared error between predicted and ground truth images. If you desire it, you could add further metrics in the training file in order to check also them. \n",
    "\n",
    "In the following cells I plot only one chart because I considered only 'mse' as stated above and I will show a visual evaluation of predicted images with respect to the real one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BIS_model import build_model\n",
    "from train import execute_training\n",
    "from utils import plot_history\n",
    "from data import create_generators\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I need to initialize a model and either train it or start from pre-trained weights. I will plot the chart of the history only if I will train again the model, otherwise no history is present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape=(32, 32, 1), n_ch=128, blocks=3, conv_per_b=2)\n",
    "pre_trained_model = True\n",
    "pre_trained_weights = '../weights/best_weights.h5'\n",
    "\n",
    "if pre_trained_model:\n",
    "    model.load_weights(pre_trained_weights)\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='mse', metrics=['mean_squared_error'])\n",
    "else:\n",
    "    history, metrics = execute_training(model, 'new_train', start_weights=None, weights_cp_path='../weights/cp_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pre_trained_model:\n",
    "    plot_history(history, metrics, save_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point I evaluate the model using the 'evaluate' function with the test generator and I compute the MSE average score on 20 thousand samples randomly created 10 times as suggested in the problem statement.    \n",
    "I also compute the standard deviation in order to see how closer are the different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need only the test generator\n",
    "_, _, test_generator = create_generators(1, 1, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(model, test_generator):\n",
    "    res = []\n",
    "    for i in range(10):\n",
    "        metrics = model.evaluate(test_generator, steps=20)\n",
    "        res.append(metrics[1])\n",
    "    return res\n",
    "\n",
    "pred_res = compute_mse(model, test_generator)\n",
    "print(f\"The average value of MSE is: {np.mean(pred_res)}\")\n",
    "print(f\"The standard deviation is: {np.std(pred_res)}\")\n",
    "print(f\"The min value of MSE is: {np.min(pred_res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reached a ***minimum*** MSE score of ***0.000296*** on a test set of 20 thousand samples.\n",
    "\n",
    "The MSE average score on 20 thousand samples randomly created ten times by the test generator is **0.000299994**.      \n",
    "\n",
    "The standard deviation between the different tests is **2.39e-06**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_test(generator, model, n_images=1):\n",
    "    x_test, y_test = next(generator)\n",
    "\n",
    "    for i in range(n_images):\n",
    "        print(f'\\n --------- IMAGE {i+1} --------- \\n')\n",
    "        # Create the grid\n",
    "        fig = plt.figure(figsize=(12, 8))\n",
    "        grid = plt.GridSpec(2, 4, wspace=0.2, hspace=0.1)\n",
    "\n",
    "        # Mixed image\n",
    "        mixed_fig = fig.add_subplot(grid[0, 1])\n",
    "        mixed_fig.axis('off')\n",
    "        mixed_fig.set_title('Mixed image')\n",
    "        mixed_fig.imshow(x_test[i], cmap='gray', interpolation='nearest')\n",
    "\n",
    "        # Reconstructed image\n",
    "        recons_fig = fig.add_subplot(grid[1, :2])\n",
    "        recons_fig.axis('off')\n",
    "        recons_fig.set_title('Reconstructed image')\n",
    "        # Transform the test image into the right shape\n",
    "        x, y = np.expand_dims(np.reshape(x_test[i], (32,32,1)), 0), y_test[i]   \n",
    "        res = model.predict(x)\n",
    "        recons_fig.imshow(np.reshape(res, (32,64)), cmap='gray', interpolation='nearest')\n",
    "        \n",
    "        # Groundtruth image\n",
    "        orig_fig = fig.add_subplot(grid[1, 2:])\n",
    "        orig_fig.axis('off')\n",
    "        orig_fig.set_title('Groundtruth image')\n",
    "        orig_fig.imshow(y, cmap='gray', interpolation='nearest')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_test(test_generator, model, n_images=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visual evaluation is not very meaningful because the model is able to split the mixed images also if the MSE value is much higher than the reached one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68e619da2bb6c9c1c5635ce51e881f0e5a6276e056aa86aa8e805e883a507c13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
